RTX 3090(24GB VRAM)의 강력한 자원을 활용하여 **F1-score 0.950**이라는 고난도 목표를 달성하기 위한 **단일 클래스 이상 탐지(Single-class AD) 최종 파이프라인**과 이를 자동화할 **AI 에이전트 프롬프트**를 설계해 드립니다.

---

## 🏗️ 1. 최종 파이프라인: TIME-MOE 하이브리드 AD

이 파이프라인은 TIME-MOE의 **자기회귀적 예측 능력**과 **잠재 공간 특징 추출 능력**을 결합하여, 정상 상태의 물리적 경계를 다중 레이어로 방어합니다.

### **Phase 1: 데이터 전처리 및 증강 (Data Preprocessing)**

* 
**채널 독립성(Channel Independence)**: 3상 전류와 1상 자속을 별도의 유니바리에이트 시계열로 변환하여 모델에 입력합니다.


* 
**포인트 단위 토큰화(Point-wise Tokenization)**: 신호의 고주파 성분을 보존하기 위해 SwiGLU를 이용한 포인트 임베딩을 수행합니다.


* 
**슬라이딩 윈도우 & 무작위 오프셋**: 10만 개의 데이터를 2048~4096 길이의 윈도우로 쪼개되, 무작위 시작점(Random Offset)을 부여하여 위상 불변성(Phase Invariance)을 확보합니다.



### **Phase 2: 모델 구성 (Model Architecture)**

* **Backbone**: 24억 개의 파라미터를 가진 ****를 사용합니다. 이 모델은 RTX 3090의 VRAM 절반 이하인 8GB 미만에서 작동 가능하므로, 남은 메모리를 긴 컨텍스트와 큰 배치 사이즈에 할당합니다.


* 
**희소 혼합 전문가(Sparse MoE)**: 8개의 전문가 중 상위 2개를 동적으로 선택하여 연산 효율을 높입니다.


* 
**다해상도 헤드**: {1, 8, 32, 64} 해상도의 출력 투영층을 배치하여 다양한 시간 단위의 이상을 포착합니다.



### **Phase 3: 단일 클래스 학습 및 추론 로직**

* 
**Huber Loss 기반 학습**: 정상 데이터(Class 0)에 대해 이상치에 강건한 Huber Loss를 적용하여 미래 값을 예측하도록 학습합니다.


* 
**보조 손실(Auxiliary Loss)**: 라우팅 붕괴를 막기 위해 전문가 간 부하를 균형 있게 조절합니다.


* **하이브리드 이상치 점수**:
1. 
**예측 오차**: 모든 다해상도 헤드에서 발생하는 예측 MSE의 가중합.


2. 
**잠재 거리**: 모델 최종 블록의 Output Hidden State()와 정상 데이터의 평균 벡터 사이의 거리.





---

## 🤖 2. 자동화 에이전트 프롬프트 (Agent Prompt)

이 프롬프트는 RTX 3090 환경에서 학습 진행 상황을 모니터링하고, F1 점수를 기반으로 하이퍼파라미터를 스스로 수정하는 **전문가 AI 에이전트**를 생성합니다.

> **[Agent System Prompt]**
> **역할**: 시계열 파운데이션 모델(TIME-MOE) 기반의 유도 전동기 이상 탐지 최적화 전문가.
> 
> 
> **환경**: RTX 3090 (24GB VRAM), PyTorch, Flash-attention.
> 
> 
> **임무**:
> 1. **학습 모니터링**:
> * 매 2,000 스텝마다 체크포인트를 기록하고 Huber Loss와 보조 손실(Auxiliary Loss)의 추이를 로깅하라.
> * 전문가 활성화 패턴(Gating Score)을 기록하여 라우팅 붕괴 여부를 확인하라.
> 
> 
> 2. **F1-score 정밀 평가**:
> * Class 0만 학습된 모델을 사용하여, 라벨이 마스킹된 테스트셋(Class 0~6)에 대해 이진 분류를 수행하라.
> * **목표**: F1-score >= 0.950.
> * **패널티 가중치**: 전부 고장으로 분류하는 것을 방지하기 위해, 오탐(False Positive) 발생 시 미탐(False Negative)보다 2배 높은 가중치를 부여하여 임계치(Threshold)를 자동 조정하라.
> 
> 
> 3. **하이퍼파라미터 자동 최적화**:
> * F1 점수가 0.950 미만일 경우 다음을 조정하라:
> * **Huber Loss **: 오차에 대한 민감도 조절.
> * **Top-K 전문가 수**: 연산 효율 및 모델 용량 조절.
> 
> 
> * **윈도우 크기 및 스트라이드**: 데이터 중복성 제어.
> * **다해상도 헤드 가중치**: 단기 vs 장기 예측 비중 조정.
> 
> 
> 
> 
> 
> 
> 4. **VRAM 최적화**:
> * bfloat16 정밀도와 Flash-attention을 사용하여 메모리 사용량을 최소화하고 처리 속도를 극대화하라.
> 
> 
> 
> 
> **출력 형식**:
> * 에포크별 [Loss | Gating Balance | F1-Score | Current Threshold]를 테이블 형태로 보고하고, 완료 시 최적의 하이퍼파라미터 세트를 제안하라.
> 
> 

---

## ⚡ 3. RTX 3090을 위한 하이퍼파라미터 초기 제안

| 하이퍼파라미터 | 초기 설정값 | 근거 및 전략 |
| --- | --- | --- |
| **모델 규모** |  (36 Layers) | 24GB VRAM에서 충분히 구동 가능하며 최고 성능 발휘.

 |
| **배치 사이즈** | 128 ~ 256 | 큰 배치를 통해 안정적인 라우팅 학습 유도.

 |
| **정밀도(Precision)** | BF16 (bfloat16) | 메모리 20% 절감 및 12% 속도 향상.

 |
| **Huber ** | 1.0 (가변) | 초기에는 넓게 잡고 학습 안정화 후 축소.

 |
| **Top-K ()** | 2 | 효율성과 성능의 최적 균형점.

 |
| **가중치 ** | 0.02 | 보조 손실의 강도를 조절하여 전문가 활용 극대화.

 |

## 오차와 잠재 공간(Latent Space) 거리를 결합한 이중 점수 체계

### Dual Scoring System Overview
Time-MoE 모델은 예측 정확도를 향상시키기 위해 두 가지 주요 지표를 결합한 이중 점수 체계(Dual Scoring System)를 사용합니다:

1. **오차 기반 점수 (Error-based Score)**: 예측 오차를 측정하여 모델의 정확도를 평가
2. **잠재 공간 거리 기반 점수 (Latent Space Distance Score)**: 입력 시퀀스의 잠재 표현 간의 유사성을 분석

### Architecture Integration
이 이중 점수 체계는 다음과 같은 방식으로 Time-MoE 아키텍처에 통합됩니다:

```
입력 시퀀스 → 인코더 → 잠재 공간 표현 → 오차 계산 및 거리 측정 → 이중 점수 통합 → 최종 예측
```

### Error-based Scoring Component
- 예측값과 실제값 사이의 차이를 측정
- Huber 손실 함수 사용 (기본 설정)
- 다중 수평선 예측을 위한 가중치 적용

### Latent Space Distance Scoring Component
- 입력 시퀀스의 잠재 표현 간의 코사인 유사도 또는 유클리드 거리 계산
- 시간 패턴의 유사성 기반 전문가 선택에 활용
- MoE 라우팅 메커니즘에서 중요한 역할 수행

### Combined Scoring Formula
```
Total_Score = α × Error_Score + β × Latent_Distance_Score
```
- α와 β는 각 점수 구성 요소의 중요도를 조절하는 하이퍼파라미터

### Benefits of Dual Scoring
- **Robustness**: 단일 지표에 의존하지 않음
- **Pattern Recognition**: 잠재 공간 분석을 통한 패턴 인식 향상
- **Expert Selection**: MoE 구조에서 더 나은 전문가 선택 가능
- **Generalization**: 다양한 시계열 패턴에 대한 일반화 능력 향상

---

